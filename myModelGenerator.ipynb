{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b37287a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, TimeDistributed\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Dropout, Attention\n",
    "from tensorflow.keras.models import Model\n",
    "import re\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80a59315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import glob\n",
    "\n",
    "file_pattern = f\"data/conve*.txt\"\n",
    "def extract_number(file_name):\n",
    "    match = re.search(r'\\d+', file_name)\n",
    "    return int(match.group()) if match else None\n",
    "\n",
    "matching_files = glob.glob(file_pattern)\n",
    "filtered_files = [file for file in matching_files if 0 <= extract_number(file) <= 50]\n",
    "\n",
    "print(len(filtered_files))\n",
    "print(len(matching_files))\n",
    "input_texts = []\n",
    "output_texts = []\n",
    "df = pd.DataFrame()\n",
    "for file_path in matching_files:\n",
    "    with open(file_path, \"r\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    examples = content.split(\"assistant: \")\n",
    "\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "\n",
    "    i = 1\n",
    "    for example in examples[1:]:\n",
    "        for _ in range(100):\n",
    "            input_match = re.search(f\"Input{i}: (.+?)\\n\", example)\n",
    "            output_match = re.search(f\"Output{i}: (.+?)\\n\", example)\n",
    "\n",
    "            if input_match and output_match:\n",
    "                outputs_value = output_match.group(1)\n",
    "                inputs_value = re.sub(r'Output.*', r'', input_match.group(1))\n",
    "                inputs.append(inputs_value)\n",
    "                outputs.append(outputs_value)\n",
    "                output_texts.append(outputs_value)\n",
    "                input_texts.append(inputs_value)\n",
    "            i += 1  \n",
    "    newDf = pd.DataFrame({\"input\": inputs, \"output\": outputs})\n",
    "    df = pd.concat([df, newDf], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4380aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4412\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Attention, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "max_input_seq_len = max(len(seq) for seq in input_texts)\n",
    "max_output_seq_len = max(len(seq) for seq in output_texts)\n",
    "\n",
    "input_tokenizer = Tokenizer(filters='', char_level=False)\n",
    "output_tokenizer = Tokenizer(filters='', char_level=False)\n",
    "\n",
    "input_tokenizer.fit_on_texts(input_texts)\n",
    "print(len(input_tokenizer.word_index))\n",
    "output_tokenizer.fit_on_texts(output_texts)\n",
    "\n",
    "encoder_input_seqs = input_tokenizer.texts_to_sequences(input_texts)\n",
    "decoder_input_seqs = output_tokenizer.texts_to_sequences(output_texts)\n",
    "decoder_target_seqs = [seq[1:] for seq in decoder_input_seqs]\n",
    "\n",
    "encoder_input_seqs = pad_sequences(encoder_input_seqs, maxlen=max_input_seq_len, padding='post')\n",
    "decoder_input_seqs = pad_sequences(decoder_input_seqs, maxlen=max_output_seq_len, padding='post')\n",
    "decoder_target_seqs = pad_sequences(decoder_target_seqs, maxlen=max_output_seq_len, padding='post')\n",
    "\n",
    "latent_dim = 256\n",
    "\n",
    "encoder_inputs = Input(shape=(max_input_seq_len,))\n",
    "encoder_embedding = Embedding(input_dim=len(input_tokenizer.word_index) + 1, output_dim=latent_dim, mask_zero=True)(encoder_inputs)\n",
    "encoder_lstm, state_h, state_c = LSTM(latent_dim, return_sequences=True, return_state=True)(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(max_output_seq_len,))\n",
    "decoder_embedding = Embedding(input_dim=len(output_tokenizer.word_index) + 1, output_dim=latent_dim, mask_zero=True)(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "\n",
    "attention = Attention()([decoder_outputs, encoder_lstm])\n",
    "context = Concatenate(axis=-1)([attention, decoder_outputs])\n",
    "\n",
    "decoder_dense = Dense(len(output_tokenizer.word_index) + 1, activation='softmax')\n",
    "decoder_outputs = decoder_dense(context)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1123ea4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-23 00:09:16.790418: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:693] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"110\" frequency: 1800 num_cores: 8 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 6291456 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "996/996 [==============================] - 907s 907ms/step - loss: 3.3109 - accuracy: 0.4130\n",
      "Epoch 2/10\n",
      "996/996 [==============================] - 889s 892ms/step - loss: 2.0946 - accuracy: 0.5574\n",
      "Epoch 3/10\n",
      "996/996 [==============================] - 874s 877ms/step - loss: 1.5810 - accuracy: 0.6198\n",
      "Epoch 4/10\n",
      "996/996 [==============================] - 878s 881ms/step - loss: 1.2243 - accuracy: 0.6722\n",
      "Epoch 5/10\n",
      "996/996 [==============================] - 863s 867ms/step - loss: 0.9609 - accuracy: 0.7149\n",
      "Epoch 6/10\n",
      "996/996 [==============================] - 865s 868ms/step - loss: 0.7588 - accuracy: 0.7583\n",
      "Epoch 7/10\n",
      "996/996 [==============================] - 863s 867ms/step - loss: 0.6098 - accuracy: 0.7996\n",
      "Epoch 8/10\n",
      "996/996 [==============================] - 865s 869ms/step - loss: 0.4977 - accuracy: 0.8334\n",
      "Epoch 9/10\n",
      "996/996 [==============================] - 859s 863ms/step - loss: 0.4122 - accuracy: 0.8602\n",
      "Epoch 10/10\n",
      "996/996 [==============================] - 859s 862ms/step - loss: 0.3443 - accuracy: 0.8826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ukasz/anaconda3/envs/ml_lab/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit([encoder_input_seqs, decoder_input_seqs], decoder_target_seqs, epochs=10, batch_size=1)\n",
    "\n",
    "model.save('my_model_files_0-50_dim_256.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc8271ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-07 08:20:20.403931: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:693] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"110\" frequency: 1800 num_cores: 8 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 6291456 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 0s 179ms/step\n",
      "1/1 [==============================] - 0s 188ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 221ms/step\n",
      "1/1 [==============================] - 0s 215ms/step\n",
      "1/1 [==============================] - 0s 196ms/step\n",
      "1/1 [==============================] - 0s 207ms/step\n",
      "1/1 [==============================] - 0s 183ms/step\n",
      "1/1 [==============================] - 0s 233ms/step\n",
      "1/1 [==============================] - 0s 197ms/step\n",
      "1/1 [==============================] - 0s 253ms/step\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "1/1 [==============================] - 0s 187ms/step\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 197ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 178ms/step\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "1/1 [==============================] - 0s 187ms/step\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "1/1 [==============================] - 0s 195ms/step\n",
      "1/1 [==============================] - 0s 179ms/step\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "1/1 [==============================] - 0s 189ms/step\n",
      "Input sentence: Look for the highest score in a game leaderboard\n",
      "Predicted output: ['the --> list] you set your time with a new number] b -- no --> a b -- yes --> c[learn from setbacks and keep going] c --> d[end]']\n"
     ]
    }
   ],
   "source": [
    "#model4 is the best\n",
    "model = load_model('my_model4.h5')\n",
    "\n",
    "# input_sentence = \"Scan a list of prices until finding the one with the highest value\"\n",
    "input_sentence = \"Look for the highest score in a game leaderboard\"\n",
    "# input_sentence = \"Practice speaking, writing, and listening in each language.\"\n",
    "\n",
    "input_sequence = input_tokenizer.texts_to_sequences([input_sentence])\n",
    "padded_input_sequence = pad_sequences(input_sequence, maxlen=max_input_seq_len, padding='post')\n",
    "\n",
    "decoded_sequence = np.zeros((1, max_output_seq_len))\n",
    "\n",
    "for i in range(max_output_seq_len):\n",
    "    predictions = model.predict([padded_input_sequence, decoded_sequence])\n",
    "    predicted_token_index = np.argmax(predictions[0, i, :])\n",
    "    decoded_sequence[0, i] = predicted_token_index\n",
    "\n",
    "    if predicted_token_index == 0: \n",
    "        break\n",
    "\n",
    "output_sentence = output_tokenizer.sequences_to_texts([decoded_sequence[0]])\n",
    "\n",
    "print(\"Input sentence:\", input_sentence)\n",
    "print(\"Predicted output:\", output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51c57f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 21:22:27.627862: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:693] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"110\" frequency: 1800 num_cores: 8 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 6291456 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2293/2293 [==============================] - 581s 252ms/step - loss: 3.4928 - accuracy: 0.4325\n",
      "Epoch 2/10\n",
      "2293/2293 [==============================] - 566s 247ms/step - loss: 3.1967 - accuracy: 0.4574\n",
      "Epoch 3/10\n",
      "2293/2293 [==============================] - 565s 247ms/step - loss: 2.9853 - accuracy: 0.4779\n",
      "Epoch 4/10\n",
      "2293/2293 [==============================] - 567s 247ms/step - loss: 2.8127 - accuracy: 0.4947\n",
      "Epoch 5/10\n",
      "2293/2293 [==============================] - 562s 245ms/step - loss: 2.6676 - accuracy: 0.5065\n",
      "Epoch 6/10\n",
      "2293/2293 [==============================] - 561s 245ms/step - loss: 2.5374 - accuracy: 0.5198\n",
      "Epoch 7/10\n",
      "2293/2293 [==============================] - 559s 244ms/step - loss: 2.4211 - accuracy: 0.5340\n",
      "Epoch 8/10\n",
      "2293/2293 [==============================] - 560s 244ms/step - loss: 2.3187 - accuracy: 0.5451\n",
      "Epoch 9/10\n",
      "2293/2293 [==============================] - 563s 245ms/step - loss: 2.2252 - accuracy: 0.5559\n",
      "Epoch 10/10\n",
      "2293/2293 [==============================] - 562s 245ms/step - loss: 2.1381 - accuracy: 0.5655\n"
     ]
    }
   ],
   "source": [
    "existing_model = load_model('my_model_latent_dim_16.h5')\n",
    "existing_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "existing_model.fit([encoder_input_seqs, decoder_input_seqs], decoder_target_seqs, epochs=10, batch_size=1)\n",
    "existing_model.save('my_model_latent_dim_64.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81368dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)        [(None, 420)]                0         []                            \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)        [(None, 653)]                0         []                            \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)     (None, 420, 16)              70608     ['input_3[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)     (None, 653, 16)              123488    ['input_4[0][0]']             \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)               [(None, 420, 16),            2112      ['embedding_2[0][0]']         \n",
      "                              (None, 16),                                                         \n",
      "                              (None, 16)]                                                         \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)               [(None, 653, 16),            2112      ['embedding_3[0][0]',         \n",
      "                              (None, 16),                            'lstm_2[0][1]',              \n",
      "                              (None, 16)]                            'lstm_2[0][2]']              \n",
      "                                                                                                  \n",
      " attention_1 (Attention)     (None, 653, 16)              0         ['lstm_3[0][0]',              \n",
      "                                                                     'lstm_2[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 653, 32)              0         ['attention_1[0][0]',         \n",
      " )                                                                   'lstm_3[0][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 653, 7718)            254694    ['concatenate_1[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 453014 (1.73 MB)\n",
      "Trainable params: 453014 (1.73 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = load_model('my_model_latent_dim_16.h5')\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
