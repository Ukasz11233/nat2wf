{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b37287a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, TimeDistributed\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Dropout, Attention\n",
    "from tensorflow.keras.models import Model\n",
    "import re\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80a59315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A[Start] --> B[Search the string] B --> C{Most repeated character found?} C -- No --> B C -- Yes --> D[Remove all occurrences] D --> E[End]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import glob\n",
    "\n",
    "file_pattern = f\"data/conve*.txt\"\n",
    "matching_files = glob.glob(file_pattern)\n",
    "\n",
    "input_texts = []\n",
    "output_texts = []\n",
    "df = pd.DataFrame()\n",
    "for file_path in matching_files:\n",
    "    with open(file_path, \"r\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    examples = content.split(\"assistant: \")\n",
    "\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "\n",
    "    i = 1\n",
    "    for example in examples[1:]:\n",
    "        for _ in range(100):\n",
    "            input_match = re.search(f\"Input{i}: (.+?)\\n\", example)\n",
    "            output_match = re.search(f\"Output{i}: (.+?)\\n\", example)\n",
    "\n",
    "            if input_match and output_match:\n",
    "                # outputs_value = \"flowchart TD \\n\\t\" + output_match.group(1)\n",
    "                outputs_value = output_match.group(1)\n",
    "                # outputs_value = format_mermaid_code(outputs_value)\n",
    "                inputs_value = re.sub(r'Output.*', r'', input_match.group(1))\n",
    "                inputs.append(inputs_value)\n",
    "                outputs.append(outputs_value)\n",
    "                output_texts.append(outputs_value)\n",
    "                input_texts.append(inputs_value)\n",
    "            i += 1  \n",
    "    newDf = pd.DataFrame({\"input\": inputs, \"output\": outputs})\n",
    "    df = pd.concat([df, newDf], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4380aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-19 00:17:08.112468: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:693] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"110\" frequency: 1800 num_cores: 8 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 6291456 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2293/2293 [==============================] - 3314s 1s/step - loss: 4.0419 - accuracy: 0.3982\n",
      "Epoch 2/10\n",
      "2293/2293 [==============================] - 3232s 1s/step - loss: 2.7156 - accuracy: 0.5170\n",
      "Epoch 3/10\n",
      "2293/2293 [==============================] - 3190s 1s/step - loss: 2.0679 - accuracy: 0.5756\n",
      "Epoch 4/10\n",
      "2293/2293 [==============================] - 3179s 1s/step - loss: 1.5552 - accuracy: 0.6305\n",
      "Epoch 5/10\n",
      "2293/2293 [==============================] - 3183s 1s/step - loss: 1.1518 - accuracy: 0.6904\n",
      "Epoch 6/10\n",
      "2293/2293 [==============================] - 3184s 1s/step - loss: 0.8397 - accuracy: 0.7575\n",
      "Epoch 7/10\n",
      "2293/2293 [==============================] - 3190s 1s/step - loss: 0.6248 - accuracy: 0.8116\n",
      "Epoch 8/10\n",
      "2293/2293 [==============================] - 3186s 1s/step - loss: 0.4748 - accuracy: 0.8543\n",
      "Epoch 9/10\n",
      "2293/2293 [==============================] - 3188s 1s/step - loss: 0.3558 - accuracy: 0.8904\n",
      "Epoch 10/10\n",
      "2293/2293 [==============================] - 3186s 1s/step - loss: 0.2835 - accuracy: 0.9137\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Attention, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "# Define the maximum sequence length for input and output sequences\n",
    "max_input_seq_len = max(len(seq) for seq in input_texts)\n",
    "max_output_seq_len = max(len(seq) for seq in output_texts)\n",
    "\n",
    "# Define the input and output tokenizers\n",
    "input_tokenizer = Tokenizer(filters='', char_level=False)\n",
    "output_tokenizer = Tokenizer(filters='', char_level=False)\n",
    "\n",
    "# Fit the input tokenizer on the preprocessed input sequences\n",
    "input_tokenizer.fit_on_texts(input_texts)\n",
    "\n",
    "# Fit the output tokenizer on the preprocessed output sequences\n",
    "output_tokenizer.fit_on_texts(output_texts)\n",
    "\n",
    "# Convert the input and output sequences to numerical format\n",
    "encoder_input_seqs = input_tokenizer.texts_to_sequences(input_texts)\n",
    "decoder_input_seqs = output_tokenizer.texts_to_sequences(output_texts)\n",
    "decoder_target_seqs = [seq[1:] for seq in decoder_input_seqs]\n",
    "\n",
    "# Pad the input and output sequences to fixed length\n",
    "encoder_input_seqs = pad_sequences(encoder_input_seqs, maxlen=max_input_seq_len, padding='post')\n",
    "decoder_input_seqs = pad_sequences(decoder_input_seqs, maxlen=max_output_seq_len, padding='post')\n",
    "decoder_target_seqs = pad_sequences(decoder_target_seqs, maxlen=max_output_seq_len, padding='post')\n",
    "\n",
    "# Define the model architecture\n",
    "latent_dim = 256\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_input_seq_len,))\n",
    "encoder_embedding = Embedding(input_dim=len(input_tokenizer.word_index) + 1, output_dim=latent_dim, mask_zero=True)(encoder_inputs)\n",
    "encoder_lstm, state_h, state_c = LSTM(latent_dim, return_sequences=True, return_state=True)(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(max_output_seq_len,))\n",
    "decoder_embedding = Embedding(input_dim=len(output_tokenizer.word_index) + 1, output_dim=latent_dim, mask_zero=True)(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "\n",
    "# Attention layer\n",
    "attention = Attention()([decoder_outputs, encoder_lstm])\n",
    "context = Concatenate(axis=-1)([attention, decoder_outputs])\n",
    "\n",
    "# Dense layer for prediction\n",
    "decoder_dense = Dense(len(output_tokenizer.word_index) + 1, activation='softmax')\n",
    "decoder_outputs = decoder_dense(context)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit([encoder_input_seqs, decoder_input_seqs], decoder_target_seqs, epochs=10, batch_size=1)\n",
    "\n",
    "# Save the model\n",
    "model.save('my_model4.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc8271ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-19 10:05:01.859640: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:693] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"110\" frequency: 1800 num_cores: 8 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 6291456 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 223ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 158ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 216ms/step\n",
      "1/1 [==============================] - 0s 213ms/step\n",
      "1/1 [==============================] - 0s 203ms/step\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 204ms/step\n",
      "1/1 [==============================] - 0s 158ms/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 213ms/step\n",
      "1/1 [==============================] - 0s 178ms/step\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "1/1 [==============================] - 0s 203ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "Input sentence: Scan a list of prices until finding the one with the highest value\n",
      "Predicted output: ['the first name] b --> c[choose the desired number] c --> d[iterate through the words] d --> e[apply toothpaste to the desired location] e --> f[add the timer] f --> d d -- no --> g[end]']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the trained model\n",
    "model = load_model('my_model4.h5')\n",
    "\n",
    "# Define the input sentence\n",
    "input_sentence = \"Scan a list of prices until finding the one with the highest value\"\n",
    "\n",
    "# Preprocess the input sentence\n",
    "input_sequence = input_tokenizer.texts_to_sequences([input_sentence])\n",
    "padded_input_sequence = pad_sequences(input_sequence, maxlen=max_input_seq_len, padding='post')\n",
    "\n",
    "# Initialize the decoded sequence\n",
    "decoded_sequence = np.zeros((1, max_output_seq_len))\n",
    "\n",
    "# Use the model to predict the output sequence\n",
    "for i in range(max_output_seq_len):\n",
    "    predictions = model.predict([padded_input_sequence, decoded_sequence])\n",
    "    predicted_token_index = np.argmax(predictions[0, i, :])\n",
    "    decoded_sequence[0, i] = predicted_token_index\n",
    "\n",
    "    if predicted_token_index == 0:  # End of sequence token\n",
    "        break\n",
    "\n",
    "# Convert the predicted sequence back to text\n",
    "output_sentence = output_tokenizer.sequences_to_texts([decoded_sequence[0]])\n",
    "\n",
    "print(\"Input sentence:\", input_sentence)\n",
    "print(\"Predicted output:\", output_sentence)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
